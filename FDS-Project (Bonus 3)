{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5632428,"sourceType":"datasetVersion","datasetId":3238659},{"sourceId":10698524,"sourceType":"datasetVersion","datasetId":6629799},{"sourceId":10699210,"sourceType":"datasetVersion","datasetId":6630267}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font face=\"Times New Roman\" size=5>\n<div dir=rtl align=\"center\">\n<font face=\"Times New Roman\" size=5>\nIn The Name of God\n</font>\n<br>\n<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n<br>\n<font face=\"Times New Roman\" size=4 align=center>\nSharif University of Technology - Department of Electrical Engineering\n</font>\n<br>\n<font color=\"#008080\" size=6>\nResearch Assistant\n</font>\n<hr/>\n<font color=\"#800080\" size=5>\nLong-term recordings of motor and premotor cortical spiking activity during reaching in monkeys\n<br>\n</font>\n<font size=5>\nDr. Khalaj\n<br>\n</font>\n<font size=4>\nSpring 2025\n<br>\n</font>\n<hr>\n<font face=\"Times New Roman\" size=4 align=center>\nMatin Mb \n</font>\n<br>\n<hr>\n</div></font>","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# !pip install igraph\n# !pip install cdlib\n# !pip install bayanpy leidenalg wurlitzer infomap ASLPAw pyclustering\n# !pip install keybert4\n# !pip install torch_geometric\n# !pip install keybert\n# !pip install skorch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom joblib import Parallel, delayed\nimport networkx as nx\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport random\nfrom cdlib import algorithms\nfrom sklearn.cluster import SpectralClustering\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport faiss\nimport torch\nimport spacy\nimport warnings\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer\nfrom keybert import KeyBERT\nfrom transformers import pipeline\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport os\nimport pickle\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sentence_transformers import SentenceTransformer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport ast\nimport numpy as np\nimport scipy.stats as stats\nfrom wordcloud import WordCloud\nimport gc\nimport seaborn as sns\nimport statsmodels.api as sm\nimport networkx as nx\nfrom collections import defaultdict\nfrom networkx.algorithms.community import greedy_modularity_communities\n\nimport community.community_louvain as community_louvain  # Louvain method\nfrom sklearn.cluster import SpectralClustering\nimport igraph as ig\nfrom cdlib.algorithms import infomap\n\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm\n\nimport os\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch_geometric.nn as pyg_nn\nimport torch_geometric.utils as pyg_utils\nfrom torch_geometric.data import Data\n\nfrom collections import Counter, defaultdict\n\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score\nfrom sklearn.metrics import jaccard_score\n\nfrom sklearn.cluster import KMeans\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"nechbamohammed/research-papers-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset (replace 'your_dataset.csv' with actual file)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Modified","metadata":{}},{"cell_type":"code","source":"# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\ndf['references'] = df['references'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\ndf['num_references'] = df['references'].apply(lambda x: len(x))\n\ndf['authors'] = df['authors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\ndf['num_authors'] = df['authors'].apply(lambda x: len(x))\n\ndf['title'] = df['title'].apply(lambda x: x if isinstance(x, str) else '')\ndf['len_title'] = df['title'].apply(lambda x: len(x))\n\ndf['abstract'] = df['abstract'].apply(lambda x: x if isinstance(x, str) else '')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1 - EDA\n","metadata":{}},{"cell_type":"markdown","source":"### Basic","metadata":{}},{"cell_type":"markdown","source":"● Create 3 bar charts. For each bar chart draw the number of publications in a given year range. Do this for the ranges 1937-1950, 1950-1970, 1970-1990. Compare the three.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load dataset (replace 'your_dataset.csv' with actual file)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Assuming the dataset has a column named 'year' with publication years\ndf = df.dropna(subset=['year'])  # Drop missing year values\ndf['year'] = df['year'].astype(int)  # Convert to integer if needed\n\n# Define year ranges\nranges = {\n    '1937-1950': (1937, 1950),\n    '1950-1970': (1950, 1970),\n    '1970-1990': (1970, 1990)\n}\n\n# Create bar charts\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, (label, (start, end)) in enumerate(ranges.items()):\n    subset = df[(df['year'] >= start) & (df['year'] < end)]\n    counts = subset['year'].value_counts().sort_index()  # Count publications per year\n\n    axes[i].bar(counts.index, counts.values, color='skyblue')\n    axes[i].set_title(f'Publications ({label})')\n    axes[i].set_xlabel('Year')\n    axes[i].set_ylabel('Count')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Comparison:\n\nThese three histograms compare the number of publications over three different time periods: **1937-1950**, **1950-1970**, and **1970-1990**. Here are the key observations:\n\n1. **Publications (1937-1950)**\n   - The publication count is relatively low.\n   - The highest count barely exceeds **3 publications per year**.\n   - There are several years with **zero or very few publications**, indicating that research output was sparse and inconsistent.\n\n2. **Publications (1950-1970)**\n   - There is a **significant increase** in publication count.\n   - The trend shows **a steady upward growth**, with some years experiencing a surge.\n   - By the end of this period (around 1970), the publication count reaches **over 200 per year**.\n\n3. **Publications (1970-1990)**\n   - There is **exponential growth** in publications.\n   - By the late 1980s, publication count **exceeds 2500 per year**, which is a dramatic increase compared to the previous periods.\n   - The increase is consistent, showing a strong upward trend in research output.\n\n\nThis comparison highlights the rapid acceleration in research publications over time, particularly after 1950. ","metadata":{}},{"cell_type":"markdown","source":"● Create a bar chart of the number of references over the years.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport ast  # To convert string representation of lists into actual lists\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert 'references' column from string to list\ndf['references'] = df['references'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n\n# Count the number of references per paper\ndf['num_references'] = df['references'].apply(len)\n\n# Group by year and sum the number of references\nyearly_references = df.groupby('year')['num_references'].sum()\n\n# Plot bar chart\nplt.figure(figsize=(12, 6))\nplt.bar(yearly_references.index, yearly_references.values, color='steelblue')\nplt.xlabel('Year')\nplt.ylabel('Total Number of References')\nplt.title('Number of References Over the Years')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.show()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Create a bar chart of the number of authors over the years.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport ast  # To safely convert string lists into actual lists\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert 'authors' column from string to list\ndf['authors'] = df['authors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n\n# Count the number of authors per paper\ndf['num_authors'] = df['authors'].apply(len)\n\n# Group by year and sum the number of authors\nyearly_authors = df.groupby('year')['num_authors'].sum()\n\n# Plot bar chart\nplt.figure(figsize=(12, 6))\nplt.bar(yearly_authors.index, yearly_authors.values, color='cornflowerblue')\nplt.xlabel('Year')\nplt.ylabel('Total Number of Authors')\nplt.title('Number of Authors Over the Years')\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\nplt.show()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Find the Pearson correlation coefficient and Spearman Rank correlation coefficient between the number of authors and number of references.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport ast\nfrom scipy.stats import spearmanr\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert 'authors' and 'references' columns from string to list\ndf['authors'] = df['authors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\ndf['references'] = df['references'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n\n# Extract numerical features\ndf['num_authors'] = df['authors'].apply(len)  # Count authors per paper\ndf['num_references'] = df['references'].apply(len)  # Count references per paper\ndf['n_citation'] = df['n_citation'].astype(float)  # Ensure citations are numerical\n\n# Compute Pearson correlation matrix\npearson_corr = df[['num_authors', 'num_references', 'n_citation']].corr(method='pearson')\nprint(\"Pearson Correlation Coefficients:\\n\", pearson_corr)\n\n# Compute Spearman correlation matrix\nspearman_corr = df[['num_authors', 'num_references', 'n_citation']].corr(method='spearman')\nprint(\"\\nSpearman Rank Correlation Coefficients:\\n\", spearman_corr)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"So it is 0.056028 and 0.087212.","metadata":{}},{"cell_type":"markdown","source":"● Find the Pearson and Spearman correlation coefficient between the number of authors and number of citations.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom scipy.stats import pearsonr, spearmanr\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert 'authors' column from string to list\ndf['authors'] = df['authors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\n\n# Count number of authors per paper\ndf['num_authors'] = df['authors'].apply(len)\n\n# Ensure citations are numeric\ndf['n_citation'] = pd.to_numeric(df['n_citation'], errors='coerce')\n\n# Drop NaN values (if any)\ndf = df.dropna(subset=['num_authors', 'n_citation'])\n\n# Compute Pearson correlation coefficient\npearson_corr, pearson_pval = pearsonr(df['num_authors'], df['n_citation'])\nprint(f\"Pearson Correlation: {pearson_corr:.4f}, p-value: {pearson_pval:.4f}\")\n\n# Compute Spearman rank correlation coefficient\nspearman_corr, spearman_pval = spearmanr(df['num_authors'], df['n_citation'])\nprint(f\"Spearman Correlation: {spearman_corr:.4f}, p-value: {spearman_pval:.4f}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Draw a bar chart of the title length over the years.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Ensure 'year' is numeric\ndf['year'] = pd.to_numeric(df['year'], errors='coerce')\n\n# Compute title length (number of characters)\ndf['title_length'] = df['title'].astype(str).apply(len)\n\n# Group by year to calculate average and total title length\nyearly_stats = df.groupby('year')['title_length'].agg(['mean', 'sum'])\n\n# Plot both metrics\nfig, ax1 = plt.subplots(figsize=(12, 6))\n\n# Plot average title length\nax1.bar(yearly_stats.index, yearly_stats['mean'], color='skyblue', edgecolor='black', label='Average Title Length')\nax1.set_xlabel(\"Year\")\nax1.set_ylabel(\"Average Title Length (Characters)\", color='blue')\nax1.tick_params(axis='y', labelcolor='blue')\n\n# Create second y-axis for total title length\nax2 = ax1.twinx()\nax2.plot(yearly_stats.index, yearly_stats['sum'], color='red', marker='o', linestyle='-', label='Total Title Length')\nax2.set_ylabel(\"Total Title Length (Characters)\", color='red')\nax2.tick_params(axis='y', labelcolor='red')\n\n# Titles and grid\nplt.title(\"Average & Total Title Length Over the Years\")\nax1.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Show plot\nplt.show()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Draw a wordcloud of the abstracts.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport gc  # Garbage collector for memory optimization\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Sample 10% of the data for efficiency\nsampled_df = df.sample(frac=0.1, random_state=42)\n\n# Combine sampled abstracts into a single text\ntext = \" \".join(str(abstract) for abstract in sampled_df['abstract'].dropna())\n\n# Generate WordCloud\nwordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"viridis\").generate(text)\n\n# Display WordCloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Word Cloud of Abstracts\")\nplt.show()\n\n# Clean up memory\ndel sampled_df, text, wordcloud\ngc.collect()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Find a fitting correlation coefficient between the title length of each paper with the title\nlength of the papers it references.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import pearsonr, spearmanr\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Ensure 'year' is numeric and 'references' is properly formatted\ndf['year'] = pd.to_numeric(df['year'], errors='coerce')\ndf['references'] = df['references'].apply(lambda x: eval(x) if isinstance(x, str) else [])\n\n# Compute title length\ndf['title_length'] = df['title'].astype(str).apply(len)\n\n# Create a dictionary mapping paper IDs to title lengths\ntitle_length_dict = df.set_index('id')['title_length'].to_dict()\n\n# Compute the average title length of referenced papers\ndef avg_referenced_title_length(refs):\n    ref_lengths = [title_length_dict[ref] for ref in refs if ref in title_length_dict]\n    return np.mean(ref_lengths) if ref_lengths else np.nan\n\ndf['avg_ref_title_length'] = df['references'].apply(avg_referenced_title_length)\n\n# Drop rows with NaN values in 'avg_ref_title_length' (papers with no valid references)\ndf_cleaned = df.dropna(subset=['avg_ref_title_length'])\n\n# Compute Pearson and Spearman correlation\npearson_corr, _ = pearsonr(df_cleaned['title_length'], df_cleaned['avg_ref_title_length'])\nspearman_corr, _ = spearmanr(df_cleaned['title_length'], df_cleaned['avg_ref_title_length'])\n\n# Display results\nprint(f\"Pearson Correlation: {pearson_corr:.4f}\")\nprint(f\"Spearman Correlation: {spearman_corr:.4f}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Find the top 10 authors with the most publications.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert authors column from string to list if needed\ndf['authors'] = df['authors'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n\n# Flatten author names and count occurrences\nauthor_counts = Counter(author for authors_list in df['authors'].dropna() for author in authors_list)\n\n# Get the top 10 authors\ntop_authors = author_counts.most_common(10)\n\n# Convert to DataFrame for better visualization\ntop_authors_df = pd.DataFrame(top_authors, columns=['Author', 'Publication Count'])\n\n# Display results\nprint(top_authors_df)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Find the top 10 authors with the most citations.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom collections import defaultdict\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert authors column from string to list if needed\ndf['authors'] = df['authors'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n\n# Dictionary to store total citations per author\nauthor_citations = defaultdict(int)\n\n# Accumulate citation count for each author\nfor _, row in df.dropna(subset=['authors', 'n_citation']).iterrows():\n    for author in row['authors']:\n        author_citations[author] += row['n_citation']\n\n# Get the top 10 authors by total citations\ntop_authors_citations = sorted(author_citations.items(), key=lambda x: x[1], reverse=True)[:10]\n\n# Convert to DataFrame for better visualization\ntop_authors_citations_df = pd.DataFrame(top_authors_citations, columns=['Author', 'Total Citations'])\n\n# Display results\nprint(top_authors_citations_df)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Find the top 10 papers with the most references.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert 'references' column from string to list if needed\ndf['references'] = df['references'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n\n# Count the number of references for each paper\ndf['num_references'] = df['references'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n\n# Get the top 10 papers with the most references\ntop_papers_references = df.nlargest(10, 'num_references')[['title', 'num_references']]\n\n# Display results\nprint(top_papers_references)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Find the top 10 papers with the most citations within the dataset.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Get the top 10 papers with the most citations\ntop_papers_citations = df.nlargest(10, 'n_citation')[['title', 'n_citation']]\n\n# Display results\nprint(top_papers_citations)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"● Find a way to see how well the number of publications can predict the number of\ncitations for a given author.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Convert 'authors' column from string to list if needed\ndf['authors'] = df['authors'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n\n# Initialize a dictionary to accumulate publication counts and citations per author\nauthor_data = {}\n\n# Aggregate the data\nfor _, row in df.dropna(subset=['authors', 'n_citation']).iterrows():\n    citation_count = row['n_citation']\n    for author in row['authors']:\n        if author not in author_data:\n            author_data[author] = {'publications': 0, 'citations': 0}\n        author_data[author]['publications'] += 1\n        author_data[author]['citations'] += citation_count\n\n# Convert the aggregated data into a DataFrame\nauthor_summary = pd.DataFrame(author_data).T\nauthor_summary['publications'] = author_summary['publications'].astype(int)\nauthor_summary['citations'] = author_summary['citations'].astype(int)\n\n# Prepare the data for regression\nX = author_summary[['publications']]  # Number of publications as the predictor\ny = author_summary['citations']  # Number of citations as the target\n\n# Create and fit a linear regression model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Make predictions\ny_pred = model.predict(X)\n\n# Calculate R² and MSE\nr2 = r2_score(y, y_pred)\nmse = mean_squared_error(y, y_pred)\n\n# Print the model evaluation metrics\nprint(f\"R²: {r2:.4f}\")\nprint(f\"Mean Squared Error: {mse:.4f}\")\n\n# Plot the results\nplt.scatter(X, y, color='blue', label='Actual Citations')\nplt.plot(X, y_pred, color='red', label='Regression Line')\nplt.xlabel('Number of Publications')\nplt.ylabel('Number of Citations')\nplt.title('Number of Publications vs. Number of Citations for Authors')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2 Network Analysis","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1 Citation Network (Paper-Paper Network)\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport ast\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\n# Process columns to ensure proper data types and handle missing values\ndf['references'] = df['references'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\ndf['num_references'] = df['references'].apply(len)\n\ndf['authors'] = df['authors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\ndf['num_authors'] = df['authors'].apply(len)\n\ndf['title'] = df['title'].apply(lambda x: x if isinstance(x, str) else '')\ndf['len_title'] = df['title'].apply(len)\n\ndf['abstract'] = df['abstract'].apply(lambda x: x if isinstance(x, str) else '')\n\n# Sample 1% of the dataset for faster processing\ndf_sampled = df.sample(frac=0.01, random_state=42)\n\n# Initialize a directed graph\nG = nx.DiGraph()\n\n# Add edges for each reference in the sampled dataset\nfor _, row in df_sampled.iterrows():\n    paper_id = row['id']\n    references = row['references']\n    for ref in references:\n        # Add edge only if reference exists in the sampled dataset\n        if ref in df_sampled['id'].values:  \n            G.add_edge(ref, paper_id)\n\n# Ensure 'year' column is of integer type for proper handling\ndf_sampled['year'] = df_sampled['year'].astype(int)\nyears = sorted(df_sampled['year'].unique())\n\n# List to store clustering coefficients for each year\nclustering_coeffs = []\n\n# Compute clustering coefficient for subgraphs based on years\nfor year in years:\n    # Create subgraph for papers published until the current year\n    subgraph_papers = df_sampled[df_sampled['year'] <= year]['id']\n    subgraph = G.subgraph(subgraph_papers)\n\n    # Calculate clustering coefficient if subgraph has more than 1 node\n    if len(subgraph) > 1:\n        clustering_coeff = nx.average_clustering(subgraph)\n    else:\n        clustering_coeff = 0  # Assign zero if not enough nodes\n\n    clustering_coeffs.append(clustering_coeff)\n\n# Plot clustering coefficient over time\nplt.figure(figsize=(10, 6))\nplt.plot(years, clustering_coeffs, marker='o', linestyle='-', color='b')\nplt.xlabel('Year')\nplt.ylabel('Clustering Coefficient')\nplt.title('Clustering Coefficient Over Time (Sampled Data)')\nplt.grid(True)\nplt.xticks(years, rotation=45)  # Adjust for better readability\nplt.tight_layout()  # Adjust layout to avoid label cutoff\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\n\n# Compute average path length and diameter of the graph (only for connected components)\nif nx.is_connected(G.to_undirected()):\n    avg_path_length = nx.average_shortest_path_length(G)\n    diameter = nx.diameter(G.to_undirected())\n    print(f\"Average Path Length: {avg_path_length}\")\n    print(f\"Diameter: {diameter}\")\nelse:\n    print(\"The graph is not connected, skipping the computation of average path length and diameter.\")\n    avg_path_length = 0\n    diameter = 0\n    print(f\"Average Path Length: {avg_path_length}\")\n    print(f\"Diameter: {diameter}\")\n    \n\n# Compute PageRank to identify influential papers\npagerank = nx.pagerank(G, alpha=0.85)\n\n# Sort PageRank values in descending order to find top influential papers\nsorted_pagerank = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)\n\n# Get top 10 influential papers based on PageRank\ntop_10_papers = sorted_pagerank[:10]\nprint(\"\\nTop 10 Influential Papers (Based on PageRank):\")\nfor rank, (paper_id, pr_value) in enumerate(top_10_papers, start=1):\n    print(f\"{rank}. Paper ID: {paper_id}, PageRank: {pr_value:.6f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.2 Co-authorship Network (Author-Author Network)\n","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport matplotlib.pyplot as plt\n\ndef co_authorship_net(df, frac=1):\n    # Sample the dataset for faster processing\n    df_sampled = df.sample(frac=frac, random_state=42)\n    G = nx.Graph()\n\n    # Build the co-authorship network by adding edges between authors\n    for _, row in df_sampled.iterrows():\n        authors = row['authors']\n        for i in range(len(authors)):\n            for j in range(i + 1, len(authors)):  # Avoid self-loops\n                G.add_edge(authors[i], authors[j])\n    \n    return G, df_sampled\n\n# Generate the co-authorship network\nG, df_sampled = co_authorship_net(df, 0.005)\n\n# Ensure 'year' column is of integer type\ndf_sampled['year'] = df_sampled['year'].astype(int)\nyears = sorted(df_sampled['year'].unique())\n\n# List to store network density over time\ndensities = []\n\n# Compute network density for each year\nfor year in years:\n    # Get authors for papers published up to and including the current year\n    subgraph_authors = df_sampled[df_sampled['year'] <= year]['authors'].explode().unique()\n    \n    # Create subgraph from the co-authorship network for those authors\n    subgraph = G.subgraph(subgraph_authors)\n\n    # Calculate density if the subgraph has more than 1 node\n    if len(subgraph) > 1:\n        density = nx.density(subgraph)\n    else:\n        density = 0  # Assign zero if not enough nodes\n\n    densities.append(density)\n\n# Plot Network Density Over Time\nplt.figure(figsize=(10, 6))\nplt.plot(years, densities, marker='o', linestyle='-', color='b')\nplt.xlabel('Year')\nplt.ylabel('Network Density')\nplt.title('Co-authorship Network Density Over Time')\nplt.grid(True)\nplt.xticks(years, rotation=45)  # Adjust x-ticks for better readability\nplt.tight_layout()  # Adjust layout to avoid label cutoff\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Identify influential researchers using centrality measures:","metadata":{}},{"cell_type":"code","source":"degree_centrality = nx.degree_centrality(G)\n\n# Top 10 authors by each centrality measure\ntop_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n\nprint(\"\\nTop 10 Authors by Degree Centrality:\")\nfor i, (author, score) in enumerate(top_degree):\n    print(f\"{i+1}. {author}, Score: {score}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"closeness_centrality = nx.closeness_centrality(G)\n\ntop_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n\nprint(\"\\nTop 10 Authors by Closeness Centrality:\")\nfor i, (author, score) in enumerate(top_closeness):\n    print(f\"{i+1}. {author}, Score: {score}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from networkx.algorithms.community import greedy_modularity_communities\n\ncommunities = greedy_modularity_communities(G)\nprint(f\"\\nNumber of communities detected: {len(communities)}\")\n\n# Display the largest 5 communities\nfor i, community in enumerate(sorted(communities, key=len, reverse=True)[:5]):\n    print(f\"\\nCommunity {i+1} (Size: {len(community)}):\")\n    print(\", \".join(list(community)[:10]), \"...\")  ","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2.3 Venue Network (Conference-Journal Network)\n","metadata":{}},{"cell_type":"code","source":"def venue_net(df, frac=1):\n    df_sampled = df.sample(frac=frac, random_state=42)\n    df_sampled = df_sampled.dropna(subset=['venue', 'id'])\n\n    G = nx.DiGraph()\n\n    venue_dict = dict(zip(df_sampled['id'], df_sampled['venue']))  # Map paper ID to venue\n\n    for _, row in df_sampled.iterrows():\n        citing_venue = row['venue']\n        references = row['references']\n\n        for ref in references:\n            if ref in venue_dict:  # Ensure reference exists in the sampled dataset\n                cited_venue = venue_dict[ref]\n\n                if citing_venue and cited_venue and citing_venue != cited_venue:\n                    G.add_edge(citing_venue, cited_venue)  # Create edge between venues\n\n    return G, df_sampled, venue_dict\n\nG, df_sampled, venue_dict = venue_net(df, frac=0.01)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"degree_centrality = nx.degree_centrality(G)\nbetweenness_centrality = nx.betweenness_centrality(G)\n\n# Top 10 venues by degree and betweenness centrality\ntop_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\ntop_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n\nprint(\"\\nTop 10 Venues by Degree Centrality (Most Connected Venues):\")\nfor i, (venue, score) in enumerate(top_degree):\n    print(f\"{i+1}. {venue}, Score: {score}\")\n\nprint(\"\\nTop 10 Venues by Betweenness Centrality (Bridges Between Fields):\")\nfor i, (venue, score) in enumerate(top_betweenness):\n    print(f\"{i+1}. {venue}, Score: {score}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pagerank_scores = nx.pagerank(G, alpha=0.85)\n\ntop_pagerank = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\nprint(\"\\nTop 10 Most Influential Venues (PageRank):\")\nfor i, (venue, score) in enumerate(top_pagerank):\n    print(f\"{i+1}. {venue}, PageRank Score: {score}\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_sampled['year'] = df_sampled['year'].astype(int)\nyears = sorted(df_sampled['year'].unique())\nnew_connections = []\n\nfor year in years:\n    subgraph_papers = df_sampled[df_sampled['year'] <= year]['id']\n    subgraph = nx.DiGraph()\n\n    for paper_id in subgraph_papers:\n        citing_venue = venue_dict.get(paper_id, None)\n        references = df_sampled[df_sampled['id'] == paper_id]['references'].values[0]\n\n        for ref in references:\n            cited_venue = venue_dict.get(ref, None)\n            if citing_venue and cited_venue and citing_venue != cited_venue:\n                subgraph.add_edge(citing_venue, cited_venue)\n\n    new_edges = - len(subgraph.edges) + len(G.subgraph(list(subgraph.nodes)).edges)\n    new_connections.append(new_edges)\n\n# Plot Emerging Interdisciplinary Connections Over Time\nplt.figure(figsize=(10, 6))\nplt.plot(years, new_connections, marker='o', linestyle='-')\nplt.xlabel('Year')\nplt.ylabel('New Venue Connections')\nplt.title('Emerging Interdisciplinary Collaborations Over Time')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Analysis of the Emerging Interdisciplinary Collaborations Graph**\nThis graph illustrates the number of **new venue connections** over time, highlighting the evolution of interdisciplinary collaborations.\n\n#### **Key Observations**\n1. **Pre-2000 Period:**\n   - There were almost **no new venue connections** before the early 2000s.\n   - This suggests that interdisciplinary collaborations were **limited or non-existent** during this period.\n\n2. **2000-2010:**\n   - A **rapid increase** in new venue connections started around **2000**.\n   - The growth rate accelerated, reaching a **peak around 2010**, with over **20 new connections per year**.\n   - This surge indicates a major shift towards interdisciplinary research and collaboration across different venues.\n\n3. **Post-2010 Trends:**\n   - The number of new venue connections fluctuated at a high level for several years.\n   - A **declining trend** started after **2015**, with a steep drop nearing **2020**.\n   - This decline may indicate:\n     - A stabilization where most major interdisciplinary connections have already been established.\n     - A shift in research patterns, possibly towards specialized rather than interdisciplinary collaborations.\n\n### **Interpretation in Context of Venue Networks**\n- **Emerging Fields:** The peak around 2010 suggests a period where **new interdisciplinary fields** were forming.\n- **Influence of Conferences/Journals:** Identifying key venues that contributed most to these connections could reveal the most **influential hubs** in research.\n- **Current Trends:** The decline in new connections might signal a **maturing** research landscape with well-established interdisciplinary ties.\n","metadata":{}},{"cell_type":"markdown","source":"### 1.2.4. Temporal Evolution of the Citation Network\n","metadata":{}},{"cell_type":"code","source":"sample_df = df.sample(frac=0.01, random_state=42)\n\n# Construct citation network\ndef build_citation_network(df):\n    G = nx.DiGraph()\n    for _, row in df.iterrows():\n        paper_id = row['id']\n        references = row['references'] if isinstance(row['references'], list) else []\n        G.add_node(paper_id, year=row['year'], citations=row['n_citation'])\n        for ref in references:\n            G.add_edge(ref, paper_id)\n    return G\n\nG = build_citation_network(sample_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_network_density(G):\n    years = sorted(set(nx.get_node_attributes(G, 'year').values()))\n    densities = []\n\n    for year in years:\n        sub_nodes = [n for n, y in nx.get_node_attributes(G, 'year').items() if y <= year]\n        sub_G = G.subgraph(sub_nodes)\n        density = nx.density(sub_G)\n        densities.append(density)\n\n    plt.figure(figsize=(8, 5))\n    plt.plot(years, densities, marker='o')\n    plt.xlabel('Year')\n    plt.ylabel('Network Density')\n    plt.title('Citation Network Density Over Time')\n    plt.grid()\n    plt.show()\n\nplot_network_density(G)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Analysis of Citation Network Density Over Time**\nThis graph shows how the **citation network density** has evolved over time, revealing key patterns in how research papers cite each other.\n\n#### **Key Observations**\n1. **Pre-1990:**\n   - The network density remains **near zero** for several decades, suggesting that either:\n     - The number of research papers was low.\n     - Citation practices were not yet well-established.\n     - The dataset may have incomplete citation records for this period.\n\n2. **Sharp Growth (Late 1980s - Early 1990s):**\n   - A **rapid surge in network density** occurs around **1990**, reaching a peak.\n   - This suggests a period of **intense citation activity**, likely due to:\n     - The emergence of influential papers.\n     - Increased research output and improved citation indexing.\n     - Possibly new technological advancements, making it easier to track and cite previous works.\n\n3. **Decline After the Peak (Post-1990s):**\n   - After reaching a maximum around **1990-1995**, citation network density **begins to decline**.\n   - The downward trend continues steadily, meaning:\n     - The network is **expanding**, but **new citations are spread across a larger pool of papers**, reducing density.\n     - Research is diversifying, with more niche topics developing.\n     - There may be a shift toward newer papers getting cited instead of older, well-established ones.\n\n4. **Post-2010 Stabilization:**\n   - The network density **flattens at a lower level**, indicating a **mature citation network** where new papers integrate without drastically changing the structure.\n\n### **Interpretation in the Context of Citation Networks**\n- **Bursts of Influential Papers:** The peak in density suggests that during **1990-1995**, several highly influential papers were published and widely cited.\n- **Integration of New Papers:** Over time, new papers integrate into the network, but the overall citation structure becomes **less dense**, likely due to the vast increase in the number of publications.\n","metadata":{}},{"cell_type":"code","source":"def compute_citation_growth(df):\n    citation_history = []\n\n    for _, row in df.iterrows():\n        citing_paper_year = row['year']  # Year of the citing paper\n        for ref in row['references']:   # List of referenced papers\n            citation_history.append((ref, citing_paper_year))\n\n    # Convert to DataFrame\n    citation_df = pd.DataFrame(citation_history, columns=['cited_paper_id', 'citing_year'])\n    yearly_citations = citation_df.groupby(['cited_paper_id', 'citing_year']).size().reset_index(name='n_citations_yearly')\n    # Sort values for proper calculation\n    yearly_citations = yearly_citations.sort_values(by=['cited_paper_id', 'citing_year'])\n    # Compute yearly citation growth\n    yearly_citations['prev_citations'] = yearly_citations.groupby('cited_paper_id')['n_citations_yearly'].shift(1)\n    yearly_citations['growth_rate'] = yearly_citations['n_citations_yearly'] / (yearly_citations['prev_citations'] + 1)  # Avoid division by zero\n    # Merge with original paper information\n    burst_papers = yearly_citations.merge(df[['id', 'title', 'authors', 'year']], left_on='cited_paper_id', right_on='id')\n    # Sort by highest growth rate\n    burst_papers = burst_papers.sort_values(by='growth_rate', ascending=False)\n\n    return burst_papers[['id', 'title', 'authors', 'year', 'citing_year', 'n_citations_yearly', 'growth_rate']]\n\n\nburst_papers = compute_citation_growth(df.sample(frac=1, random_state=42))\nburst_papers.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_new_papers_integration(G, df, new_papers_fraction=0.1):\n    num_new_papers = int(len(df) * new_papers_fraction)\n\n    new_papers = df.head(num_new_papers)['id'].tolist()\n    centrality = nx.degree_centrality(G)\n\n    # Compare centrality of new papers with old ones\n    new_papers_centrality = {paper: centrality.get(paper, 0) for paper in new_papers}\n    old_papers_centrality = {paper: centrality.get(paper, 0) for paper in df['id'].tolist() if paper not in new_papers}\n\n    # Calculate the average centrality for new and old papers\n    avg_new_papers_centrality = sum(new_papers_centrality.values()) / len(new_papers_centrality) if new_papers_centrality else 0\n    avg_old_papers_centrality = sum(old_papers_centrality.values()) / len(old_papers_centrality) if old_papers_centrality else 0\n\n    return avg_new_papers_centrality, avg_old_papers_centrality\n\navg_new_centrality, avg_old_centrality = analyze_new_papers_integration(G, df_sampled)\n\nprint(f\"Average centrality of new papers: {avg_new_centrality}\")\nprint(f\"Average centrality of old papers: {avg_old_centrality}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Comparison of Centrality Between New and Old Papers**  \nCentrality measures the importance of a node (paper) within the citation network. The values given represent the **average centrality** of new and old papers:\n\n- **New Papers**: \\( 0.0001168 \\)\n- **Old Papers**: \\( 0.0001143 \\)\n\n#### **Key Insights**\n1. **Slightly Higher Centrality for New Papers**  \n   - The **new papers have a marginally higher centrality** than old papers.  \n   - This suggests that **new papers are integrating well into the existing citation network** and are being referenced frequently.\n  \n2. **Citation Network Expansion**  \n   - The small difference in centrality implies that new papers are **not drastically more influential** than older ones.  \n   - This suggests a **balanced** citation network where older works still play a crucial role in shaping research.\n\n3. **Possible Explanations**\n   - The network may be transitioning toward a **distributed influence model**, where many papers (both new and old) contribute to shaping research rather than a few dominating.\n   - Older papers may still receive citations, but new papers are **quickly gaining citations**, leading to a **slightly higher average centrality**.\n\n### **Conclusion**\nWhile new papers have slightly higher centrality, the difference is **small**, indicating a stable and well-integrated citation network. ","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 2 Data Extrapolation via Clustering\n","metadata":{}},{"cell_type":"markdown","source":"### 2.1 Community Detection","metadata":{}},{"cell_type":"code","source":"from joblib import Parallel, delayed\nimport networkx as nx\nfrom tqdm.notebook import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nfrom cdlib import algorithms\nfrom sklearn.cluster import SpectralClustering\nimport random\nimport numpy as np\n\n# Function to create the author-author network\ndef create_author_network(data):\n    print(\"Building the Author-Author Network...\")\n\n    # Dictionary to store edges and their weights\n    edge_weights = defaultdict(int)\n\n    # Iterate over the rows of the DataFrame\n    for _, row in tqdm(data.iterrows(), total=len(data)):\n        if pd.notnull(row['authors']):\n            authors = eval(row['authors'])  # Convert authors list from string\n\n            # Add edges between all pairs of authors\n            for i, author1 in enumerate(authors):\n                for author2 in authors[i + 1:]:\n                    # Use a sorted tuple to avoid duplicates (undirected edges)\n                    edge = tuple(sorted((author1, author2)))\n                    edge_weights[edge] += 1\n\n    # Build the graph using the edge weights\n    author_network = nx.Graph()\n    for (author1, author2), weight in edge_weights.items():\n        author_network.add_edge(author1, author2, weight=weight)\n\n    print(f\"\\nAuthor network created with {author_network.number_of_nodes()} nodes and {author_network.number_of_edges()} edges.\")\n    return author_network\n\n# Create the author network\ndata = df_sampled  # Replace with your DataFrame variable\nauthor_network = create_author_network(data)\n\n# Function to run clustering algorithms on the network\ndef run_clustering_algorithms(author_network, sample_size=5000):\n    print(f\"\\nSampling {sample_size} nodes for clustering...\")\n\n    sampled_nodes = random.sample(list(author_network.nodes), min(sample_size, author_network.number_of_nodes()))\n    subgraph = author_network.subgraph(sampled_nodes)\n\n    # Louvain Clustering\n    print(\"\\nRunning Louvain clustering...\")\n    louvain_communities = algorithms.louvain(subgraph)\n    print(f\"Louvain clustering completed: {len(louvain_communities.communities)} communities detected.\")\n\n    # Walktrap Clustering\n    print(\"\\nRunning Walktrap clustering...\")\n    walktrap_communities = algorithms.walktrap(subgraph)\n    print(f\"Walktrap clustering completed: {len(walktrap_communities.communities)} communities detected.\")\n\n    # Spectral Clustering\n    print(\"\\nRunning Spectral clustering...\")\n    adj_matrix_sparse = nx.to_scipy_sparse_array(subgraph, dtype=np.float64).toarray()\n\n    spectral_model = SpectralClustering(\n        n_clusters=10, affinity='precomputed', random_state=42, assign_labels='kmeans', n_jobs=-1\n    )\n    spectral_labels = spectral_model.fit_predict(adj_matrix_sparse)\n\n    # Organize Spectral Communities\n    spectral_communities = {label: [] for label in set(spectral_labels)}\n    for node, label in zip(subgraph.nodes, spectral_labels):\n        spectral_communities[label].append(node)\n\n    print(f\"Spectral clustering completed: {len(spectral_communities)} communities detected.\")\n    \n    return louvain_communities, walktrap_communities, spectral_communities\n\n# Run the clustering algorithms\nlouvain, walktrap, spectral = run_clustering_algorithms(author_network, sample_size=5000)\n\n# Function to evaluate the clustering algorithms\ndef evaluate_clustering_algorithms(graph, louvain_clusters, walktrap_clusters, spectral_clusters):\n    def community_clustering_coefficient(net, cluster_groups, batch_size=500):\n        nodes_set = {node for cluster in cluster_groups for node in cluster}\n        node_clustering_coeffs = nx.clustering(net, nodes=nodes_set)\n\n        def process_batch(cluster_batch):\n            results = [\n                sum(node_clustering_coeffs[node] for node in cluster if node in node_clustering_coeffs) / len(cluster)\n                for cluster in cluster_batch if cluster\n            ]\n            return results\n\n        cluster_batches = [\n            cluster_groups[i : i + batch_size] for i in range(0, len(cluster_groups), batch_size)\n        ]\n        coefficients = Parallel(n_jobs=-1)(delayed(process_batch)(batch) for batch in cluster_batches)\n        coefficients = [cc for batch_result in coefficients for cc in batch_result]\n        return sum(coefficients) / len(coefficients)\n\n    print(\"\\nEvaluating clustering algorithms...\")\n\n    louvain_cc = community_clustering_coefficient(graph, louvain_clusters.communities)\n    walktrap_cc = community_clustering_coefficient(graph, walktrap_clusters.communities)\n    spectral_cc = community_clustering_coefficient(graph, list(spectral_clusters.values()))\n\n    clustering_scores = {\n        \"Louvain\": louvain_cc,\n        \"Walktrap\": walktrap_cc,\n        \"Spectral\": spectral_cc,\n    }\n\n    best_algo = max(clustering_scores, key=clustering_scores.get)\n    print(f\"\\nClustering Evaluation Results:\")\n    for algo, score in clustering_scores.items():\n        print(f\"{algo} Clustering Coefficient: {score:.4f}\")\n    \n    print(f\"\\nBest Algorithm based on Clustering Coefficient: {best_algo}\")\n    return best_algo\n\n# Evaluate the clustering algorithms\nbest_algorithm = evaluate_clustering_algorithms(author_network, louvain, walktrap, spectral)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n## 2.1 Community Detection\n\n### 1. Author-Author Network\n\nThe first step in this analysis was to build the **author-author network**. This network connects authors based on their collaborative work, specifically the papers they have published together. Each node in the network represents an author, and an edge is drawn between two authors if they have co-authored at least one paper. In this step, the **author network** was created with the following characteristics:\n\n- **Number of nodes**: 2542 (representing 2542 authors).\n- **Number of edges**: 4173 (representing the collaborations between authors).\n\nThe data was processed in a way that each author retains information about the papers they have published, ensuring that we can analyze the network based on author relationships.\n\n### 2. Author Communities Identification\n\nTo find communities of authors within this network, three different clustering algorithms were employed. The goal was to detect groups of authors that are more densely connected within the group than with others. The algorithms used for clustering were:\n\n- **Louvain Clustering**\n- **Walktrap Clustering**\n- **Spectral Clustering**\n\nThe algorithms were applied to a **sample of 5000 nodes** to ensure that the clustering process could be completed efficiently. The results of the community detection are as follows:\n\n- **Louvain Clustering**: 741 communities detected.\n- **Walktrap Clustering**: 741 communities detected.\n- **Spectral Clustering**: 10 communities detected.\n\n### 3. Evaluation of Clustering Algorithms\n\nTo evaluate the performance of the three clustering algorithms, we used the **clustering coefficient** as the evaluation metric. The clustering coefficient measures the degree to which nodes in a graph tend to cluster together. A higher clustering coefficient suggests that the community structure is more cohesive and tightly-knit.\n\nThe clustering coefficient for each algorithm is:\n\n- **Louvain Clustering Coefficient**: 0.6735\n- **Walktrap Clustering Coefficient**: 0.6735\n- **Spectral Clustering Coefficient**: 0.7119\n\n### 4. Best Clustering Algorithm\n\nBased on the clustering coefficient values, **Spectral Clustering** was identified as the best clustering algorithm, as it produced the highest clustering coefficient of **0.7119**. This indicates that spectral clustering was able to identify more cohesive communities in the author-author network compared to the Louvain and Walktrap algorithms.\n\n### Conclusion\n\nThrough this analysis, we identified the most appropriate clustering algorithm for detecting author communities within the network. Spectral clustering provided the best results in terms of community cohesion, making it the preferred method for further analysis.\n\n","metadata":{}},{"cell_type":"code","source":"# Load dataset (replace 'your_dataset.csv' with actual filename)\ndf = pd.read_csv('/kaggle/input/research-papers-dataset/dblp-v10.csv')\n\ndf['references'] = df['references'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\ndf['num_references'] = df['references'].apply(lambda x: len(x))\n\ndf['authors'] = df['authors'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else [])\ndf['num_authors'] = df['authors'].apply(lambda x: len(x))\n\ndf['title'] = df['title'].apply(lambda x: x if isinstance(x, str) else '')\ndf['len_title'] = df['title'].apply(lambda x: len(x))\n\ndf['abstract'] = df['abstract'].apply(lambda x: x if isinstance(x, str) else '')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Naming the Communities\n","metadata":{}},{"cell_type":"code","source":"from keybert import KeyBERT\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport networkx as nx\nfrom cdlib import algorithms\nfrom collections import defaultdict, Counter\nimport json\n\n# Initialize KeyBERT model\nkw_model = KeyBERT(\"distilbert-base-nli-mean-tokens\")\ntqdm.pandas()\n\n# Load the dataset\ndef load_data(file_path):\n    print(\"\\n📂 Loading dataset...\")\n    data = pd.read_csv(file_path)\n    print(f\"✅ Dataset loaded with {data.shape[0]} rows and {data.shape[1]} columns.\")\n    return data\n\n# Extract keywords efficiently\ndef extract_keywords(text, max_keywords=5):\n    if not isinstance(text, str) or not text.strip():\n        return []\n    keywords = kw_model.extract_keywords(text, keyphrase_ngram_range=(1, 2), stop_words=\"english\", top_n=max_keywords)\n    return [kw[0] for kw in keywords]\n\n# Build the author network efficiently\ndef create_author_network(data):\n    print(\"\\n📡 Building Author-Author Network...\")\n    author_network = nx.Graph()\n    edge_count = 0  # Track number of edges added\n\n    for authors in tqdm(data[\"authors\"].dropna().map(eval), desc=\"Processing Authors\"):\n        for i, author1 in enumerate(authors):\n            for author2 in authors[i + 1:]:\n                if author_network.has_edge(author1, author2):\n                    author_network[author1][author2]['weight'] += 1\n                else:\n                    author_network.add_edge(author1, author2, weight=1)\n                    edge_count += 1\n\n    print(f\"✅ Network built successfully: {author_network.number_of_nodes()} nodes, {edge_count} edges.\")\n    return author_network\n\n# Run community detection\ndef detect_communities(graph):\n    print(\"\\n🔍 Running Louvain community detection...\")\n    communities = algorithms.louvain(graph)\n    print(f\"✅ Detected {len(communities.communities)} communities.\")\n    return communities\n\n# Map authors to communities\ndef map_authors_to_communities(communities):\n    print(\"\\n🗺 Mapping authors to their communities...\")\n    mapping = {author: community_id for community_id, community in enumerate(communities) for author in community}\n    print(f\"✅ Mapped {len(mapping)} authors to communities.\")\n    return mapping\n\n# Map papers to communities\ndef map_papers_to_communities(data, author_to_community):\n    print(\"\\n📑 Assigning papers to communities based on author affiliations...\")\n    paper_to_community = defaultdict(set)\n\n    for paper_id, authors in zip(data['id'], data['authors'].dropna().map(eval)):\n        for author in authors:\n            if author in author_to_community:\n                paper_to_community[paper_id].add(author_to_community[author])\n\n    print(f\"✅ Mapped {len(paper_to_community)} papers to communities.\")\n    return paper_to_community\n\n# Aggregate keywords for communities\ndef aggregate_community_keywords(data, paper_to_community):\n    print(\"\\n📊 Aggregating keywords for each community...\")\n    community_keywords = defaultdict(list)\n\n    for paper_id, communities in paper_to_community.items():\n        keywords = data.loc[data['id'] == paper_id, 'keywords'].values\n        if len(keywords) > 0:\n            for community in communities:\n                community_keywords[community].extend(eval(keywords[0]))\n\n    result = {community: dict(Counter(keywords).most_common(10)) for community, keywords in community_keywords.items()}\n    print(f\"✅ Aggregated keywords for {len(result)} communities.\")\n    return result\n\n# Assign community names\ndef assign_community_names(community_keywords):\n    print(\"\\n🏷 Assigning descriptive names to communities...\")\n    names = {community: \" & \".join(list(keywords.keys())[:3]) for community, keywords in community_keywords.items()}\n    print(f\"✅ Assigned names to {len(names)} communities.\")\n    return names\n\n# Main execution\n\ndata = df_sampled\ndata[\"text\"] = data[\"abstract\"].fillna(\"\") + \" \" + data[\"title\"].fillna(\"\")\nprint(\"📉 Sampling 50% of dataset for faster processing...\")\nsampled_data = data.sample(frac=0.5, random_state=42)\nprint(f\"✅ Sampled {sampled_data.shape[0]} rows.\")\n\nprint(\"\\n📝 Extracting keywords for sampled papers...\")\nsampled_data[\"keywords\"] = sampled_data[\"text\"].progress_apply(lambda x: extract_keywords(x))\nprint(\"✅ Keyword extraction completed!\")\n\nsampled_data.to_csv(\"sampled_papers_with_keywords.csv\", index=False)\nprint(\"\\n💾 Sampled dataset with keywords saved as 'sampled_papers_with_keywords.csv'.\")\n\n# Load Data\nfile_path = \"sampled_papers_with_keywords.csv\"\ndata = load_data(file_path)\n\n# Create network and run clustering\nauthor_network = create_author_network(data)\nlouvain_communities = detect_communities(author_network)\nauthor_to_community = map_authors_to_communities(louvain_communities.communities)\npaper_to_community = map_papers_to_communities(data, author_to_community)\ncommunity_keywords = aggregate_community_keywords(data, paper_to_community)\ncommunity_names = assign_community_names(community_keywords)\n\n# Save results\ndata.to_csv(\"sampled_papers_with_communities_and_keywords.csv\", index=False)\nprint(\"\\n💾 Final dataset saved as 'sampled_papers_with_communities_and_keywords.csv'.\")\n\nwith open(\"community_names.json\", \"w\") as f:\n    json.dump(community_names, f)\nprint(\"💾 Community names saved as 'community_names.json'.\")\n\n# Display sample output\nprint(\"\\n🎯 Sample Community Names:\")\nfor community, name in list(community_names.items())[:5]:\n    print(f\"🏘 Community {community}: {name}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2 Naming the Communities\n\n#### 1. Keyword Extraction from Titles and Abstracts\n\nIn this task, we aimed to extract relevant keywords from the **title** and **abstract** of each paper. To accomplish this, we used **KeyBERT**, a tool based on **BERT embeddings**, to extract keyphrases that represent the central themes of each paper. For each paper, we aggregated these keyphrases into a single list of keywords. A maximum of **5 keywords** was selected for each paper to ensure relevance and avoid overfitting.\n\n#### 2. Associating Papers with Communities\n\nAfter extracting the keywords for each paper, we associated each paper with the corresponding **community of authors**. Since a paper may be authored by individuals from multiple communities, we ensured that each paper could belong to one or more communities. This association was done by mapping each paper to the set of communities its authors belong to.\n\n#### 3. Aggregating Keywords for Communities\n\nTo obtain the representative keywords for each community, we aggregated the keywords of all papers that belong to the community. If a paper belonged to multiple communities, its keywords were included in the aggregation of each of these communities.\n\n**Aggregation Method**:\n- We collected the keywords for each paper and aggregated them across all papers associated with a specific community.\n- To ensure we capture the most common and relevant keywords for each community, we used the **Counter** method to count the occurrences of each keyword across papers in the community. The **top 10 most frequent keywords** were selected to represent the community.\n\nThe result was a set of the most representative keywords for each community.\n\n#### 4. Assigning Names to Communities\n\nNaming the communities was done by selecting the **top 3 keywords** from the aggregated list for each community. These keywords were concatenated into a community name, offering a brief but descriptive label for each community. The choice of names was based on the following process:\n\n- **Relevance**: We selected keywords that are most relevant to the papers within the community.\n- **Clarity**: We ensured that the community name was understandable and accurately represented the primary themes in the community.\n\n**Example Community Names**:\n1. **Community 252**: \"storytelling enabled & storytelling collaboratively & posted internet\"\n2. **Community 253**: \"transformation generator & negation operators & operators integrative\"\n3. **Community 130**: \"structuring holistic & strategies implemented & making decisions\"\n4. **Community 254**: \"alzheimer disease & simulation alzheimer & dealing alzheimer\"\n5. **Community 10**: \"web advantage & rapid growth & impressive results\"\n\nThese names were generated based on the **most frequent** and **relevant keywords** extracted from the titles and abstracts of the papers in each community.\n\n#### Conclusion\n\nBy extracting keywords from the abstracts and titles of papers and aggregating them across communities, we were able to assign meaningful names to the communities based on their thematic focus. This process provides a way to summarize large, complex networks of authors into manageable and interpretable groupings, facilitating a deeper understanding of the collaborative landscape in the dataset.\n","metadata":{}},{"cell_type":"markdown","source":"The identification of research communities begins by extracting key terms from paper abstracts and titles using KeyBERT, ensuring the selection of the most relevant and representative keywords. Each paper is then assigned to research communities, which are determined through Louvain clustering of the author-author network. Since authors often contribute to multiple fields, papers are dynamically linked to multiple communities based on their authorship, accurately capturing interdisciplinary collaborations.\n\nTo define the thematic focus of each community, extracted keywords from individual papers are aggregated and analyzed, allowing for the identification of dominant research themes. Finally, each community is assigned a meaningful label by selecting the three most frequently occurring keywords. This structured approach ensures that communities are clearly defined, making their core research focus easily interpretable.\n\n","metadata":{}},{"cell_type":"markdown","source":"### 2.3 Paper-Paper Clustering via Embedding\n","metadata":{}},{"cell_type":"code","source":"# Load embedding model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Embed abstract, title, and keywords\ndef embed_text(row):\n    text = row[\"title\"] + \" \" + row[\"abstract\"] + \" \" + \" \".join(row[\"keywords\"])\n    return model.encode(text)\n\ndf_sampled[\"embedding\"] = df_sampled.apply(embed_text, axis=1)\nembeddings = np.vstack(df_sampled[\"embedding\"].values)\n\n# Perform Clustering\nnum_clusters = 5  \nkmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\ndf_sampled[\"cluster\"] = kmeans.fit_predict(embeddings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate Clustering\nsilhouette_avg = silhouette_score(embeddings, df_sampled[\"cluster\"])\ndbi_score = davies_bouldin_score(embeddings, df_sampled[\"cluster\"])\n\n# Unique venue values\nunique_venues = df_sampled[\"venue\"].unique()\n\n# Compute Jaccard Similarity Index with venue clustering\nvenue_to_cluster = {venue: i for i, venue in enumerate(unique_venues)}\ndf_sampled[\"venue_cluster\"] = df_sampled[\"venue\"].map(venue_to_cluster)\n\njaccard = jaccard_score(df_sampled[\"cluster\"], df_sampled[\"venue_cluster\"], average=\"macro\")\n\n# Print results\nprint(f\"Silhouette Score: {silhouette_avg}\")\nprint(f\"Davies-Bouldin Index: {dbi_score}\")\nprint(f\"Unique Venues: {len(unique_venues)}\")\nprint(f\"Jaccard Similarity with Venue Clustering: {jaccard}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results\n\n- **Silhouette Score**: 0.038\n   - This score measures the quality of the clusters. A score closer to 1 means that the clusters are well-separated, while a score closer to -1 indicates overlapping clusters. The low silhouette score of 0.038 suggests that the clusters are not very well-defined, which may indicate that the clustering method did not result in distinct, well-separated groups.\n\n- **Davies-Bouldin Index**: 3.89\n   - The Davies-Bouldin Index is another metric for evaluating cluster quality. A lower score is better, indicating that clusters are compact and well-separated. With a value of 3.89, the clusters might be less well-separated, which aligns with the low silhouette score observed.\n\n- **Unique Venues**: 410\n   - There are 410 unique venues in the dataset, which can serve as another feature for clustering, but the poor performance of clustering metrics suggests that using just the venue as a clustering feature might lead to better results.\n\n- **Jaccard Similarity with Venue Clustering**: 0.00019\n   - The Jaccard Similarity Index between the clusters and the venue-based clustering is extremely low, indicating that the clusters formed by the embedding model are very different from the clusters based on venues. This could suggest that the venue may not be a very informative feature for clustering based on the abstract and title of the papers.\n\n### Improvement\n\n1. **Clustering Improvement**: \n   - Try different clustering algorithms like DBSCAN, Agglomerative Clustering, or hierarchical clustering, as they may be more suitable for the data's characteristics.\n   - Experiment with other embeddings, such as BERTopic or domain-specific models, to capture more meaningful features of the papers.\n","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# 3 Citation Regressor","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T00:54:55.041868Z","iopub.execute_input":"2025-02-09T00:54:55.042183Z","iopub.status.idle":"2025-02-09T00:54:55.045929Z","shell.execute_reply.started":"2025-02-09T00:54:55.042159Z","shell.execute_reply":"2025-02-09T00:54:55.045023Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\n# Embedding text data using SentenceTransformer\ndef embed_texts_from_dataframe(df, model_name=\"all-MiniLM-L6-v2\"):\n    \"\"\"Embeds text data (title + abstract) from a dataframe using a pre-trained SentenceTransformer model.\"\"\"\n    df[\"combined_text\"] = df[\"title\"] + \" \" + df[\"abstract\"]\n    model = SentenceTransformer(model_name)\n    embeddings = [model.encode(text, convert_to_tensor=True) for text in tqdm(df[\"combined_text\"], desc=\"Embedding texts\")]\n    return torch.stack(embeddings)\n\n# Splitting data into training and validation sets, including authors\ndef split_data_for_training(embeddings, authors_list, target_values, test_size=0.2, random_state=42):\n    \"\"\"Splits the dataset into training and validation sets along with corresponding author information.\"\"\"\n    np.random.seed(random_state)\n    indices = np.random.permutation(len(embeddings))\n    split_index = int(test_size * len(indices))\n    train_indices = indices[split_index:]\n    val_indices = indices[:split_index]\n\n    X_train, y_train = embeddings[train_indices], target_values[train_indices]\n    X_val, y_val = embeddings[val_indices], target_values[val_indices]\n    authors_train, authors_val = authors_list[train_indices], authors_list[val_indices]\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    return (X_train, X_val,\n            torch.tensor(y_train, dtype=torch.float32, device=device),\n            torch.tensor(y_val, dtype=torch.float32, device=device),\n            authors_train, authors_val)\n\n# Save or load data from disk\ndef handle_data_storage(file_path, data=None, is_save=True):\n    \"\"\"Saves or loads data from a specified file.\"\"\"\n    storage_path = os.path.join(\"/kaggle/working/\", file_path)\n    \n    if is_save and data is not None:\n        with open(storage_path, 'wb') as f:\n            pickle.dump(data, f)\n        print(f\"Data saved to {storage_path}\")\n    elif not is_save and os.path.exists(storage_path):\n        with open(storage_path, 'rb') as f:\n            data = pickle.load(f)\n        print(f\"Data loaded from {storage_path}\")\n    else:\n        print(\"No data found for loading.\")\n        return None\n\n    return data\n\n# Multi-layer Perceptron (MLP) model for regression\nclass MLPRegressor(nn.Module):\n    def __init__(self, input_dim):\n        super(MLPRegressor, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 128)\n        self.layer2 = nn.Linear(128, 64)\n        self.output_layer = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        return self.output_layer(x)\n\n# Training or loading the model\ndef train_or_load_mlp_model(model, X_train, y_train, epochs=20, learning_rate=0.001, should_save=True, model_path=\"model.pth\"):\n    \"\"\"Trains the MLP model or loads it from the specified path if should_save is False.\"\"\"\n    model_path = os.path.join(\"/kaggle/working/\", model_path)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if not should_save and os.path.exists(model_path):\n        model.load_state_dict(torch.load(model_path))\n        model.eval()\n        print(f\"Model loaded from {model_path}.\")\n        return model\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    for epoch in tqdm(range(epochs), desc=\"Training MLP model\"):\n        model.train()\n        optimizer.zero_grad()\n        predicted_values = model(X_train)\n        loss = criterion(predicted_values, y_train.view(-1, 1))\n        loss.backward()\n        optimizer.step()\n\n    if should_save:\n        torch.save(model.state_dict(), model_path)\n        print(f\"Model saved to {model_path}.\")\n    \n    return model\n\n# Evaluate the model on the provided dataset\ndef evaluate_model_performance(model, X, y):\n    \"\"\"Evaluates the model on given data and calculates performance metrics.\"\"\"\n    model.eval()\n    with torch.no_grad():\n        y_pred = model(X).cpu().numpy().flatten()\n\n    y_true = y.cpu().numpy()\n    metrics = {\n        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n        \"MAE\": mean_absolute_error(y_true, y_pred),\n        \"R² Score\": r2_score(y_true, y_pred)\n    }\n\n    return metrics\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Embedding","metadata":{}},{"cell_type":"code","source":"# Main workflow\ndef main():\n    LOAD = True  # Set to True if loading saved data\n    \n    df_cleaned = df.dropna(subset=[\"title\", \"abstract\"])  # Clean dataframe by removing missing values\n\n    # Sample 1% of the data\n    df_sampled = df_cleaned.sample(frac=0.01, random_state=42)\n\n    data = None\n    if not LOAD:\n        embeddings = embed_texts_from_dataframe(df_sampled)\n        data = split_data_for_training(embeddings, df_sampled[\"authors\"].values, df_sampled[\"n_citation\"].values)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load or save data\nX_train, X_val, y_train, y_val, authors_train, authors_val = handle_data_storage(\"data.pkl\", data, is_save=(not LOAD))\n\n# Initialize model and train/load it\nmodel = MLPRegressor(X_train.shape[1]).to(X_train.device)\nmodel = train_or_load_mlp_model(model, X_train, y_train, should_save=True, model_path=\"mlp_model1.pth\")\n\n# Evaluate the model on training and validation sets\ntrain_metrics = evaluate_model_performance(model, X_train, y_train)\nval_metrics = evaluate_model_performance(model, X_val, y_val)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training Metrics:\", train_metrics)\nprint(\"Validation Metrics:\", val_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_json('/kaggle/input/testset/testset.json').dropna()\n\npredictions, test_metrics = evaluate_dataframe(model, df_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Test', test_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results\n\n- **Training Metrics**:\n   - **RMSE**: 141.14\n   - **MAE**: 38.35\n   - **R² Score**: -0.0796\n   - The model's performance on the training set shows that the predictions are not very accurate, as indicated by the negative R² score. This suggests that the model performs worse than a simple mean-based model.\n\n- **Validation Metrics**:\n   - **RMSE**: 146.15\n   - **MAE**: 42.76\n   - **R² Score**: -0.0936\n   - The validation set results are also poor, with a slightly worse performance compared to the training set, which may indicate overfitting or insufficient model generalization.\n\n- **Test Metrics**:\n   - **RMSE**: 941.01\n   - **MAE**: 236.40\n   - **R² Score**: 0.0674\n   - The test set performance is much worse, with an extremely high RMSE and MAE, and a very low R² score. This suggests that the model fails to generalize well to the unseen data, which may be due to the time gap between the training/validation and test sets, as well as the introduction of new concepts/authors that were not present in the training data.\n\n### Potential Reasons for Poor Performance\n\n1. **Time Gap Between Training/Validation and Test Sets**:\n   - The model may have difficulty predicting citations for the test set, especially if there is a time gap between when the training data was collected and the test set is used. New research topics, authors, and trends that were not captured in the training set may affect the accuracy of the predictions.\n\n2. **New Concepts/Authors**:\n   - The test set might contain papers from new authors or with new topics that were not present in the training set. This issue is typical in citation prediction tasks, as papers from emerging areas may not share similarities with the older, training data.\n\n### Next Steps\n\n1. **Model Improvement**:\n   - **Use of Domain-Specific Embeddings**: You might want to use embeddings specifically trained on academic texts (e.g., BERT-based models fine-tuned for academic papers) to better capture the nuances of the abstracts and titles.\n   - **Incorporating Temporal Features**: Adding temporal features like the year of publication or more detailed temporal embeddings could help the model adapt to the evolution of academic trends.\n   - **Ensemble Methods**: You could explore ensemble models (e.g., Random Forest, XGBoost) that might perform better in predicting citation counts by combining different feature sets and models.\n\n2. **Incorporate Author-Author Network**:\n   - The author-author network could be a valuable feature for predicting citations. A graph-based model that incorporates author collaboration patterns might yield better results. One approach could be to represent the author network as a graph and use graph neural networks (GNN) to learn from it.\n\n3. **Model Evaluation and Hyperparameter Tuning**:\n   - Perform hyperparameter tuning (e.g., via GridSearch or AutoML) on different regression models to find the best-performing one for your task.\n   - Given the low R² and high error metrics, it might be worthwhile to test other regression models (e.g., Random Forest, Gradient Boosting) or fine-tune the MLP architecture for better performance.\n","metadata":{}},{"cell_type":"markdown","source":"### Bonus","metadata":{}},{"cell_type":"code","source":"!pip install torch_geometric\n!pip install cdlib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:03:46.218975Z","iopub.execute_input":"2025-02-09T01:03:46.219262Z","iopub.status.idle":"2025-02-09T01:03:59.445570Z","shell.execute_reply.started":"2025-02-09T01:03:46.219233Z","shell.execute_reply":"2025-02-09T01:03:59.444587Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.11)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.9.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\nCollecting cdlib\n  Downloading cdlib-0.4.0-py3-none-any.whl.metadata (8.8 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cdlib) (4.67.1)\nRequirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from cdlib) (3.4.2)\nCollecting demon (from cdlib)\n  Downloading demon-2.0.6-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: python-louvain>=0.16 in /usr/local/lib/python3.10/dist-packages (from cdlib) (0.16)\nRequirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.13.1)\nCollecting pulp (from cdlib)\n  Downloading PuLP-2.9.0-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from cdlib) (0.12.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from cdlib) (2.2.3)\nCollecting eva-lcd (from cdlib)\n  Downloading eva_lcd-0.1.1-py3-none-any.whl.metadata (731 bytes)\nCollecting bimlpa (from cdlib)\n  Downloading bimlpa-0.1.2-py3-none-any.whl.metadata (725 bytes)\nCollecting python-igraph>=0.10 (from cdlib)\n  Downloading python_igraph-0.11.8-py3-none-any.whl.metadata (2.8 kB)\nCollecting angelcommunity (from cdlib)\n  Downloading angelcommunity-2.0.0-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from cdlib) (1.8.2)\nCollecting dynetx (from cdlib)\n  Downloading dynetx-0.3.2-py3-none-any.whl.metadata (2.9 kB)\nCollecting thresholdclustering (from cdlib)\n  Downloading thresholdclustering-1.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting python-Levenshtein (from cdlib)\n  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from cdlib) (5.24.1)\nRequirement already satisfied: igraph==0.11.8 in /usr/local/lib/python3.10/dist-packages (from python-igraph>=0.10->cdlib) (0.11.8)\nRequirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from igraph==0.11.8->python-igraph>=0.10->cdlib) (1.7.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->cdlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->cdlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->cdlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->cdlib) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->cdlib) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->cdlib) (2.4.1)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from angelcommunity->cdlib) (1.0.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bimlpa->cdlib) (3.7.5)\nRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from dynetx->cdlib) (4.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->cdlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->cdlib) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->cdlib) (2025.1)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->cdlib) (9.0.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly->cdlib) (24.2)\nRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->cdlib) (4.3.6)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->cdlib) (2.32.3)\nCollecting Levenshtein==0.26.1 (from python-Levenshtein->cdlib)\n  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein->cdlib)\n  Downloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cdlib) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->cdlib) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bimlpa->cdlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bimlpa->cdlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bimlpa->cdlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bimlpa->cdlib) (1.4.7)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bimlpa->cdlib) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bimlpa->cdlib) (3.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->cdlib) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->cdlib) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->cdlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->cdlib) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->cdlib) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->cdlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->cdlib) (2024.2.0)\nDownloading cdlib-0.4.0-py3-none-any.whl (263 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading python_igraph-0.11.8-py3-none-any.whl (9.1 kB)\nDownloading angelcommunity-2.0.0-py3-none-any.whl (10 kB)\nDownloading bimlpa-0.1.2-py3-none-any.whl (7.0 kB)\nDownloading demon-2.0.6-py3-none-any.whl (7.3 kB)\nDownloading dynetx-0.3.2-py3-none-any.whl (39 kB)\nDownloading eva_lcd-0.1.1-py3-none-any.whl (9.2 kB)\nDownloading PuLP-2.9.0-py3-none-any.whl (17.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\nDownloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading thresholdclustering-1.1-py3-none-any.whl (5.3 kB)\nDownloading rapidfuzz-3.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, pulp, demon, python-igraph, Levenshtein, python-Levenshtein, thresholdclustering, eva-lcd, dynetx, bimlpa, angelcommunity, cdlib\nSuccessfully installed Levenshtein-0.26.1 angelcommunity-2.0.0 bimlpa-0.1.2 cdlib-0.4.0 demon-2.0.6 dynetx-0.3.2 eva-lcd-0.1.1 pulp-2.9.0 python-Levenshtein-0.26.1 python-igraph-0.11.8 rapidfuzz-3.12.1 thresholdclustering-1.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pickle\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:03:59.447137Z","iopub.execute_input":"2025-02-09T01:03:59.447384Z","iopub.status.idle":"2025-02-09T01:03:59.450784Z","shell.execute_reply.started":"2025-02-09T01:03:59.447361Z","shell.execute_reply":"2025-02-09T01:03:59.450182Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"storage_path = os.path.join(\"/kaggle/input/embedd/data.pkl\")\nwith open(storage_path, 'rb') as f:\n    data = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:03:59.452149Z","iopub.execute_input":"2025-02-09T01:03:59.452338Z","iopub.status.idle":"2025-02-09T01:03:59.669786Z","shell.execute_reply.started":"2025-02-09T01:03:59.452321Z","shell.execute_reply":"2025-02-09T01:03:59.669150Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport networkx as nx\nimport os\nimport pickle\nimport numpy as np\nfrom torch_geometric.data import Data\nfrom torch import nn, optim\nfrom torch_geometric import nn as pyg_nn\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom tqdm import tqdm\n\ndef generate_author_embeddings(features, author_groups, device):\n    author_vectors = {}\n    for idx, authors in enumerate(author_groups):\n        emb = features[idx].to(device)\n        for writer in authors:\n            author_vectors.setdefault(writer, []).append(emb)\n    \n    for writer, vectors in author_vectors.items():\n        author_vectors[writer] = torch.mean(torch.stack(vectors), dim=0).to(device)\n    \n    return author_vectors\n\ndef construct_author_network(features, author_lists, targets=None, device='cpu'):\n    graph = nx.Graph()\n    author_representations = {}\n    author_index_map = {}\n    \n    for idx, authors in enumerate(author_lists):\n        feature_vec = features[idx].to(device)\n        for contributor in authors:\n            author_representations.setdefault(contributor, []).append(feature_vec)\n    \n    for i, (contributor, vectors) in enumerate(author_representations.items()):\n        author_index_map[contributor] = i\n        author_representations[contributor] = torch.mean(torch.stack(vectors), dim=0).to(device)\n        graph.add_node(contributor, vector=author_representations[contributor])\n    \n    edges = [(author_index_map[a], author_index_map[b])\n              for group in author_lists for i, a in enumerate(group) for b in group[i+1:] \n              if a in author_index_map and b in author_index_map]\n    \n    edge_idx_tensor = torch.tensor(edges, dtype=torch.long).T.to(device) if edges else torch.empty((2, 0), dtype=torch.long).to(device)\n    node_features_tensor = torch.stack([graph.nodes[node]['vector'] for node in author_representations.keys()]).to(device)\n    \n    if targets is not None:\n        label_tensor = torch.zeros(len(author_index_map)).to(device)\n        count_tensor = torch.zeros(len(author_index_map)).to(device)\n        \n        for idx, authors in enumerate(author_lists):\n            for contributor in authors:\n                node_idx = author_index_map[contributor]\n                label_tensor[node_idx] += targets[idx]\n                count_tensor[node_idx] += 1\n        \n        label_tensor /= count_tensor.clamp(min=1)\n        return Data(x=node_features_tensor, edge_index=edge_idx_tensor, y=label_tensor)\n    \n    return Data(x=node_features_tensor, edge_index=edge_idx_tensor)\n\nclass GraphNeuralNet(nn.Module):\n    def __init__(self, feature_size, hidden_size=128, output_size=1):\n        super().__init__()\n        self.gcn1 = pyg_nn.GCNConv(feature_size, hidden_size)\n        self.gcn2 = pyg_nn.GCNConv(hidden_size, hidden_size // 2)\n        self.out_layer = nn.Linear(hidden_size // 2, output_size)\n        self.activation = nn.ReLU()\n    \n    def forward(self, data):\n        node_attr, edge_idx = data.x, data.edge_index\n        node_attr = self.activation(self.gcn1(node_attr, edge_idx))\n        node_attr = self.activation(self.gcn2(node_attr, edge_idx))\n        return torch.exp(self.out_layer(node_attr)).squeeze()\n\ndef train_model(gnn, graph_data, num_epochs=200, learning_rate=0.001):\n    optimizer = optim.Adam(gnn.parameters(), lr=learning_rate)\n    loss_fn = nn.MSELoss()\n    \n    for _ in tqdm(range(num_epochs), desc=\"Training Process\"):\n        gnn.train()\n        optimizer.zero_grad()\n        predictions = gnn(graph_data)\n        loss = loss_fn(predictions, graph_data.y)\n        loss.backward()\n        optimizer.step()\n    \n    return gnn\n\ndef assess_model(gnn, graph_data):\n    gnn.eval()\n    with torch.no_grad():\n        predicted_vals = gnn(graph_data).cpu().numpy().flatten()\n    true_vals = graph_data.y.cpu().numpy()\n    return {\n        \"RMSE\": np.sqrt(mean_squared_error(true_vals, predicted_vals)),\n        \"MAE\": mean_absolute_error(true_vals, predicted_vals),\n        \"R² Score\": r2_score(true_vals, predicted_vals)\n    }\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndata_path = os.path.join(\"/kaggle/input/embedd/data.pkl\")\nwith open(data_path, 'rb') as file:\n    dataset = pickle.load(file)\n\nX_train, X_val, y_train, y_val, train_authors, val_authors = dataset\n\ngraph_train = construct_author_network(X_train, train_authors, y_train, device=device)\ngraph_val = construct_author_network(X_val, val_authors, y_val, device=device)\n\ngraph_train.y = graph_train.y.to(device)\ngraph_val.y = graph_val.y.to(device)\n\ngnn_model = GraphNeuralNet(graph_train.x.shape[1]).to(device)\ngnn_model = train_model(gnn_model, graph_train, num_epochs=1, learning_rate=0.001)\n\ntrain_results = assess_model(gnn_model, graph_train)\nprint('Training Results:', train_results)\n\nval_results = assess_model(gnn_model, graph_val)\nprint('Validation Results:', val_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-09T01:04:13.638266Z","iopub.execute_input":"2025-02-09T01:04:13.638733Z","iopub.status.idle":"2025-02-09T01:04:43.193763Z","shell.execute_reply.started":"2025-02-09T01:04:13.638682Z","shell.execute_reply":"2025-02-09T01:04:43.193023Z"}},"outputs":[{"name":"stderr","text":"Training Process: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n","output_type":"stream"},{"name":"stdout","text":"Training Results: {'RMSE': 80.49916, 'MAE': 36.726086, 'R² Score': -0.2594678280480949}\nValidation Results: {'RMSE': 74.57679, 'MAE': 48.406998, 'R² Score': -0.7255459146638112}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kill -9 2632\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Product\n","metadata":{}},{"cell_type":"code","source":"import kagglehub\nimport os\nimport pandas as pd\n\n# Download latest version\npath = kagglehub.dataset_download(\"nechbamohammed/research-papers-dataset\")\n\n# List the files inside the downloaded dataset directory\nfiles = os.listdir(path)\nprint(\"Files in dataset directory:\", files)\n\n# Assuming there's only one CSV file, construct the correct path\ncsv_file_path = os.path.join(path, files[0])  # Adjust if multiple files exist\n\n# Read the CSV file into a DataFrame\ndata = pd.read_csv(csv_file_path)\n\n# Display basic information about the DataFrame\ndata.info()\n\n# Display the first few rows of the DataFrame\ndata.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # !pip install faiss-gpu\n# !pip install faiss-cpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\ndef load_sample_data(dataset, sample_fraction=0.01):\n    sample_df = dataset.sample(frac=sample_fraction, random_state=42)\n    sample_df['authors'] = sample_df['authors'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n    sample_df[\"combined_text\"] = sample_df[\"title\"].fillna(\"\") + \" \" + sample_df[\"abstract\"].fillna(\"\")\n    return sample_df\n\ndef initialize_embeddings(papers_df):\n    print(\"  Initializing embeddings and FAISS index...\")\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n    \n    paper_content = papers_df[\"combined_text\"].tolist()\n    embeddings = embedding_model.encode(paper_content, show_progress_bar=True, convert_to_numpy=True)\n    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n    \n    dimension = embeddings.shape[1]\n    faiss_index = faiss.IndexFlatIP(dimension)\n    faiss_index.add(embeddings)\n    \n    print(f\"  FAISS index built with {faiss_index.ntotal} research papers.\")\n    return embedding_model, faiss_index\n\ndef initialize_tools():\n    print(\"  Loading KeyBERT, spaCy, and LLM model...\")\n    kw_extractor = KeyBERT(model=SentenceTransformer('all-MiniLM-L6-v2'))\n    spacy_nlp = spacy.load(\"en_core_web_sm\")\n    text_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=0 if torch.cuda.is_available() else -1)\n    return kw_extractor, spacy_nlp, text_generator\n\n\ndef retrieve_papers(query_text, embedder, faiss_idx, dataset, kw_model, nlp_model, top_k=3):\n    print(\"  Processing query...\")\n    keywords = kw_model.extract_keywords(query_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5)\n    keyword_str = \" \".join([kw for kw, _ in keywords])\n    \n    nlp_doc = nlp_model(query_text)\n    author_names = [ent.text for ent in nlp_doc.ents if ent.label_ == \"PERSON\"]\n    \n    query_embedding = embedder.encode([keyword_str], convert_to_numpy=True)\n    query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n    \n    distances, indices = faiss_idx.search(query_embedding, top_k)\n    retrieved_papers = dataset.iloc[indices[0]].copy()\n    \n    if author_names:\n        # Handle NaN or non-iterable 'authors' in dataset\n        author_filter = dataset[\"authors\"].apply(\n            lambda authors: any(\n                any(author.lower() in a.lower() for a in authors) if isinstance(authors, list) else False\n                for author in author_names\n            )\n        )\n        boosted_results = dataset[author_filter]\n        retrieved_papers = pd.concat([boosted_results, retrieved_papers]).drop_duplicates(subset=\"id\").head(top_k)\n    \n    return retrieved_papers\n\n\ndef generate_summary_report(paper_list, llm_model):\n    if paper_list.empty:\n        return \"  No relevant papers found.\"\n    \n    report_content = \"Found the following relevant research papers:\\n\"\n    for _, row in paper_list.iterrows():\n        report_content += f\"\\nTitle: {row['title']}\\nAbstract: {row['abstract']}\\nAuthors: {', '.join(row['authors'])}\\n\"\n    \n    print(\"\\n  Retrieved Articles:\")\n    print(paper_list[[\"title\", \"abstract\"]])\n    \n    prompt_text = (\n        \"You are a research assistant. Generate a concise summary report based on the following research papers. \"\n        \"Highlight the key topics, contributions, and notable authors.\\n\\n\" + report_content + \"\\n\\nSummary Report:\"\n    )\n    \n    output = llm_model(prompt_text, max_length=400, do_sample=False)\n    return output[0]['generated_text'].strip()\n\n\ndef visualize_retrieval_results(paper_df):\n    top_authors = paper_df.explode(\"authors\")[\"authors\"].value_counts().head(5)\n    \n    print(\"  Top Authors in Retrieved Papers:\")\n    for author, count in top_authors.items():\n        print(f\"{author}: {count} papers\")\n\ndef research_assistant_pipeline(user_query, data_df, sample_fraction=0.01, top_k=3):\n    papers_df = load_sample_data(data_df, sample_fraction)\n    embedder, faiss_idx = initialize_embeddings(papers_df)\n    kw_model, spacy_model, llm_model = initialize_tools()\n    \n    retrieved = retrieve_papers(user_query, embedder, faiss_idx, papers_df, kw_model, spacy_model, top_k)\n    visualize_retrieval_results(retrieved)\n    \n    final_summary = generate_summary_report(retrieved, llm_model)\n    return final_summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == '__main__':\n    user_input_query = input(\"Enter query: \")\n    research_report = research_assistant_pipeline(user_input_query, data, sample_fraction=0.01, top_k=5)\n    print(\"\\n=== Research Summary Report ===\\n\")\n    print(research_report)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install llama-index pandas faiss-cpu sentence-transformers\n!pip install faiss-cpu\n!pip install faiss-gpu\n!pip install keybert","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport faiss\nfrom sentence_transformers import SentenceTransformer\nfrom llama_index.core import VectorStoreIndex, StorageContext, Document\nimport numpy as np\nimport ast\nfrom langchain.schema import Document\nimport spacy\nfrom keybert import KeyBERT\nfrom transformers import pipeline\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def merged_search(query, top_k=2):\n    filtered_ids, filtered_texts = search_by_author(query, top_k, True)\n\n    keyword_results = search_documents(query, top_k, filtered_ids)\n\n    all_results = keyword_results\n    unique_results = list(dict.fromkeys(all_results))\n    return unique_results [:top_k]\n\ndef search_by_author(query, top_k=2, return_id = False):\n    doc = nlp(query)\n    author_name = None\n    for ent in doc.ents:\n        if ent.label_ == \"PERSON\":\n            author_name = ent.text\n            break\n\n    if not author_name:\n        return [], []\n\n    filtered_ids = []\n    filtered_texts = []\n\n    for doc in documents:\n        if (author_name+' ') in doc:\n            doc_id = doc.split(\", \")[0].split(\": \")[1]\n            filtered_ids.append(int(doc_id)-1)\n            filtered_texts.append(doc)\n    if return_id:\n        return filtered_ids[:top_k], filtered_texts[:top_k]\n\n    return filtered_texts[:top_k]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"facebook/opt-125m\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\ndef summarize_abstract2(text):\n    input_tokens = len(tokenizer.encode(text))\n    max_input_length = min(input_tokens, 512)  # Limit to model's max capacity\n    max_output_length = max(50, input_tokens // 4)  # Adjust summary length\n\n    prompt = f\"Summarize the following text concisely:\\n\\n{text}\\n\\nSummary:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=max_input_length, truncation=True)\n    outputs = model.generate(**inputs, max_length=max_output_length, num_beams=5, early_stopping=True)\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n\ndef search_and_summarize2(query, top_k=3):\n    ss = merged_search(query, top_k=top_k)\n    print(ss)\n    for s in ss:\n        print(s)\n        print(summarize_abstract(s))\n        print('-------------------------------------------------------------------------------')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"search_and_summarize2('machine learning by Mohammad')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}